\documentclass[../main.tex]{subfiles}

\begin{document}
\chapter{Concetti preliminari}
\epigraph{Sotto il teorema di Bayes, nessuna teoria è perfetta. Piuttosto, è un lavoro in corso, sempre soggetto a ulteriori perfezionamenti e prove.}{\textit{Nate Silver}}
\hfill \break
La statistica bayesiana è un sottocampo della statistica in cui la verità di un enunciato è espressa in termini di credibilià (\textit{beliefe})
o più specificatamente di \textbf{probabilità bayesiana}.
\begin{definition}[Probabilità bayesiana]
    \label{def:probabilità_bayesiana}
    La probabilità bayesiana è un'interpretazione del concetto di probabilità, in cui, anzichè la frequenza di un qualche evento, la probabilità
    viene vista come aspettativa razionale che rappresenta un stato di conoscenza o come quantificazione di una convinzione personale.
\end{definition}
\hfill \break
Nella visione bayesiana assegnamo una probabilità ad un'ipotesi, che è la probabilità che l'ipotesi sia vera.
Nell' approccio frequentista invece, l'ipotesi viene solo verificata senza che le venga assegnata una probabilità.
Per valutare la probabilità di un'ipotesi si deve fornire una \textbf{probabilità iniziale}(\textit{probabilità a priori}).
Questa a sua volta viene aggiornata con l'arrivo di nuovi dati, che vengono utilizzati per calcolare la \textbf{probabilità aggiornata}(\textit{probabilità a posteriori}).

\section{Metodologia bayesiana}
L'interpretazione bayesiana fornisce un insieme standard di procedure e formule per aggiornare la probabilità iniziale con l'arrivo di nuovi dati.
Queste procedure sono chiamate \textbf{metodologia bayesiana} e sono le seguenti:
\begin{itemize}
    \item Uso di variabili casuali, per modellare le fonti di incertezza nei modelli statistici, compresa l'incertezza risultante dalla mancanza di informazioni.
    \item La necessità di determinare la distribuzione di probabilità iniziale, tenendo conto delle informazioni disponibili.
    \item Uso ricorsivo del \textbf{teorema di Bayes} per aggiornare la probabilità iniziale con l'arrivo di nuovi dati.
    \item La probabilità che può essere assegnata ad un evento è un numero reale compreso tra 0 e 1.
\end{itemize}
La distribuzione finale di probabilità può a sua volta diventare una probabilità iniziale che può essere aggiornata con nuovi dati.
\section{Funzioni belief e probabilità}
La probabilità è un modo per esprimere numericamente un'aspettativa razionale.
Mostriamo adesso che molte proprietà che hanno le nostre aspettative numeriche sono anche valide per le probabilità.
\subsection{Funzione belief (\textit{belief function})}
\begin{definition}[Funzione belief]
    \label{def:funzione_belief}
    La funzione belief è una funzione che associa ad ogni possibile evento o enunciato un valore reale compreso tra 0 e 1.
\end{definition}
\hfill \break
Siano $F$, $G$ e $H$ tre possibili enunciati che possono sovrapporsi.
Sia $Be()$ una funzione belief che assegna un numero agli enunciati tale che maggiore è il numero assegnato, maggiore è la credibilità dell'enunciato (\textit{degree of belief}).
Ad esempio, se $Be(F) > Be(G)$ allora $F$ è più credibile di $G$.\\
Vogliamo che la funzione descriva le nostre credenze sotto certe condizioni:
\begin{itemize}
    \item $Be(F|H) > Be(G|H)$ significa che dato $H$ vero, allora è più credibile $F$ che $G$.
    \item $Be(F|G) > Be(F|H)$ significa che se siamo costretti a scommettere su $F$, lo facciamo più volentieri sotto la condizione che $G$ sia vero rispetto alla condizione che $H$ sia vero.
\end{itemize}


\begin{comment}
\section{Richiami di statistica}
\begin{definition}[Partizione di un insieme]
    Una partizione dell'insieme $H$ è una famiglia di sottoinsiemi $\{ H_{1},H_{2},\cdots H_{k}\}$ che soddisfa le seguenti proprietà:
    \begin{enumerate}
        \item $H_{i}\cap H_{j}=\emptyset$ per ogni $i\neq j$;
        \item $\bigcup_{i=1}^{k}H_{i}=H$.
    \end{enumerate}
\end{definition}
In altre parole abbiamo detto che:
\begin{itemize}
    \item La famiglia $\{ H_{1},H_{2},\cdots H_{k}\}$ è detta \textbf{disgiunta} se $H_{i}\cap H_{j}=\emptyset$ per ogni $i\neq j$.
    \item La famiglia $\{ H_{1},H_{2},\cdots H_{k}\}$ è detta \textbf{completa} se $\bigcup_{i=1}^{k}H_{i}=H$.
    \item La famiglia $\{ H_{1},H_{2},\cdots H_{k}\}$ è detta \textbf{partizione di H} se è disgiunta e completa. 
\end{itemize}
Sia $\{ H_{1},H_{2},\cdots H_{k}\}$ un partizione di $H$,$P(H)=1$ e sia $E$ un evento specifico. Allora gli assiomi di probabilità ci dicono che:
\begin{itemize}
    \item \begin{equation}\label{eq:prob tot}
        \sum_{i=1}^{k}P(H_{i})=1.
    \end{equation}
    \item \begin{equation}\label{eq:prob marg}
        P(E)=\sum_{i=1}^{k}P(E\cap H_{i})=\sum_{i=1}^{k}P(E|H_{i})P(H_{i}).
    \end{equation}
\end{itemize}
Dove \ref{eq:prob tot} è detta \textbf{Regola di probabilità totale} e \ref{eq:prob marg} è detta \textbf{Regola di probabilità marginale}.
\begin{definition}[Formula di Bayes]
\begin{equation}
    P(H_{j}|E)=\frac{P(E|H_{j})P(H_{j})}{P(E)}\stackrel{\ref{eq:prob marg}}{=}\frac{P(E|H_{j})P(H_{j})}{\sum_{i=1}^{k}P(E|H_{i})P(H_{i})}.
\end{equation}
\end{definition}
\end{comment}
\end{document}